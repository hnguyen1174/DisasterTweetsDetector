{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Installing the Huggingface's Transformers library\n","!pip install transformers"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install datasets"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# IMPORT LIBRARIES\n","\n","# Utils\n","import os                                       # Operating system operations\n","import json                                     # Working with json file\n","import re                                       # Regular expression\n","import unicodedata                              # Unicode + regular expression\n","import random                                   # Random\n","import collections                              # Counter\n","\n","# Computation\n","import pandas as pd\n","import numpy as np\n","\n","# Visualization\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# NLP\n","import nltk\n","import gensim\n","import gensim.downloader as gensim_api\n","\n","# Transformers\n","import transformers\n","from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast\n","from transformers import Trainer, TrainingArguments\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer\n","\n","# Datasets\n","from datasets import load_dataset\n","\n","# ML utils\n","from sklearn.model_selection import train_test_split\n","from sklearn import feature_extraction, naive_bayes, pipeline, manifold, preprocessing\n","from sklearn.dummy import DummyClassifier\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score\n","\n","# PyTorch\n","import torch\n","\n","# Others\n","from lime import lime_text"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dirname = '/kaggle/input/nlp-getting-started'\n","\n","train = pd.read_csv(os.path.join(dirname, 'train.csv'))\n","test = pd.read_csv(os.path.join(dirname, 'test.csv'))\n","sample_submission = pd.read_csv(os.path.join(dirname, 'sample_submission.csv'))"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_submission"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_metrics(pred):\n","    \"\"\"\n","    This function computes metrics for Transformers' fine tuning\n","    \n","    Args:\n","        pred: predictions from Transformers' Trainer\n","    \n","    Returns:\n","        A dictionary that contains metrics of interest for binary classification:\n","            (1) Accuracy\n","            (2) Precision\n","            (3) Recall\n","            (4) F1 Score\n","            (5) AUC\n","    \"\"\"\n","    labels = pred.label_ids\n","    preds = pred.predictions.argmax(-1)\n","    \n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n","    acc = accuracy_score(labels, preds)\n","    auc = roc_auc_score(labels, preds)\n","    \n","    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall, \"auc\": auc}"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tokenize(batch):\n","    \"\"\"\n","    Tokenize by batches for Transformers\n","    \"\"\"\n","    return tokenizer(batch[\"text\"], padding=True, truncation=True)"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def set_cuda_seed(seed_val=42):\n","    random.seed(seed_val)\n","    np.random.seed(seed_val)\n","    torch.manual_seed(seed_val)\n","    torch.cuda.manual_seed_all(seed_val)"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_train_texts = train.text.to_list()\n","all_train_labels = train.target.to_list()\n","\n","train_texts, val_texts, train_labels, val_labels = train_test_split(\n","    all_train_texts, all_train_labels, \n","    test_size=0.2, \n","    random_state=692\n",")"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df = pd.DataFrame(list(zip(train_texts, train_labels)),\n","                        columns =['text', 'label'])\n","\n","val_df = pd.DataFrame(list(zip(val_texts, val_labels)),\n","                      columns =['text', 'label'])"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_df = test[['id', 'text']]"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_path = '/kaggle/working/data'\n","if not os.path.exists(data_path):\n","    os.makedirs(data_path)\n","\n","train_df.to_csv(os.path.join(data_path, 'train_df.csv'), index=False)\n","val_df.to_csv(os.path.join(data_path, 'val_df.csv'), index=False)\n","\n","tweets_dataset = load_dataset('csv', data_files={'train': os.path.join(data_path, 'train_df.csv'),\n","                                                 'validation': os.path.join(data_path, 'val_df.csv')})"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_df.to_csv(os.path.join(data_path, 'test_df.csv'), index=False)\n","\n","tweets_test_dataset = load_dataset('csv', data_files = {'test': os.path.join(data_path, 'test_df.csv')})"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tweets_dataset"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tweets_test_dataset"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from transformers import AutoModelForSequenceClassification, AutoTokenizer\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model_name = 'bert-base-uncased'\n","num_labels = 2\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","tweets_encoded = tweets_dataset.map(tokenize, batched=True, batch_size=None)\n","model = (AutoModelForSequenceClassification\n","         .from_pretrained(model_name, num_labels=num_labels)\n","         .to(device))"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["set_cuda_seed()\n","\n","training_args = TrainingArguments(\n","    output_dir='./results',         \n","    num_train_epochs=2,             \n","    per_device_train_batch_size=16,  \n","    per_device_eval_batch_size=64,   \n","    warmup_steps=500,\n","    evaluation_strategy=\"epoch\",\n","    weight_decay=0.01,               \n","    logging_dir='./logs',       \n","    logging_steps=10,\n",")\n","\n","trainer = Trainer(\n","    model=model,                      \n","    args=training_args,                 \n","    train_dataset=tweets_encoded[\"train\"],\n","    eval_dataset=tweets_encoded[\"validation\"],\n","    compute_metrics=compute_metrics\n",")\n","\n","trainer.train()"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tweets_test_encoded = tweets_test_dataset.map(tokenize, batched=True, batch_size=None)"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["preds_raw = trainer.predict(tweets_test_encoded['test'])"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def softmax(x):\n","    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n","    e_x = np.exp(x - np.max(x))\n","    return e_x / e_x.sum(axis=0)"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions_prob = np.array(list(map(softmax, preds_raw.predictions)))[:,1]"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submission = pd.DataFrame(list(zip(test['id'], [i for i in map(round, predictions_prob)])),\n","                          columns = ['id', 'target'])"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import collections\n","collections.Counter(submission['target'])"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submission.to_csv('submission.csv', index=False)"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{},"execution_count":null,"outputs":[]}]}